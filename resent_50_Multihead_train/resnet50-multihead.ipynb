{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport copy\nimport timm\nfrom torch.optim import Adam, SGD\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\n## Python\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\nimport cv2\nimport time,datetime \nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n### albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class cfg:\n    num_workers=4\n    model_name = 'resnet50'\n    size = 384\n    split = 5\n    num_epochs = 12\n    lr=1e-4\n    T_0=10 # CosineAnnealingWarmRestarts\n    min_lr = 1e-6\n    batch_size = 16\n    smoothing=0.05\n    target_size=5\n    seed = 42\n    weight_decay=1e-6\n    patience = 3\n    target_size=5\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport random\n\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\n\ndef init_logger(log_file = 'F_384_resnet50_multihead_LB.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nseed_everything(cfg.seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LOGGER.info(f\"{'batch_size',cfg.batch_size}\")\nLOGGER.info(f\"{'model_name',cfg.model_name}\")\nLOGGER.info(f\"{'size',cfg.size}\")\nLOGGER.info(f\"{'num_epochs',cfg.num_epochs}\")\nLOGGER.info(f\"{'lr',cfg.lr}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_path = '/kaggle/input/cassava-leaf-disease-classification/train_images'\ntrain_file_path = '/kaggle/input/cassava-leaf-disease-classification/train.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_file_path)\nprint('length of dataset',len(df_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Stratified KFold\nskf  = StratifiedKFold(n_splits = cfg.split ,shuffle = True)\nfor n,(train_idx,val_idx) in enumerate(skf.split(df_train.image_id ,df_train.label)):\n    df_train.loc[val_idx,'stf_Kfold'] = int(n)\nprint('Number of Unique folds in dataset',df_train['stf_Kfold'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Train dataset\nclass Train_dataset(Dataset):\n    def __init__(self,df ,transform = None ):\n        \n        self.df = df\n        self.train_img_path = train_img_path\n        self.transform =transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        lab = self.df.loc[index,'label']\n        img = os.path.join(self.train_img_path,self.df.loc[index,'image_id'])\n        img = cv2.imread(img)\n        img =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(image = img)['image']\n        \n        return img,lab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_train_transform():\n    return A.Compose([\n        A.RandomResizedCrop(cfg.size,cfg.size),        \n        A.VerticalFlip(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.Transpose(p=0.5),\n        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n    \n    \n        A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0, p=1.0\n            ),\n        A.CoarseDropout(p=0.5),\n        #A.Cutout(p=0.5),\n\n        ToTensorV2(p=1.0),\n        \n    ],p=1)\n\ndef get_valid_transform():\n    return A.Compose([\n        \n        A.Resize(cfg.size,cfg.size ),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0, p=1.0\n            ),\n        ToTensorV2(p=1.0)\n        \n    ], p=1.)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AdaptiveConcatPool2d(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.average_pool = torch.nn.AdaptiveAvgPool2d((1,1))\n        self.max_pool = torch.nn.AdaptiveMaxPool2d(((1,1)))\n\n    def forward(self, x):\n        return torch.cat([self.max_pool(x), self.average_pool(x)], 1).squeeze(3).squeeze(2)\n\n\nclass cnn_resnet50(nn.Module):\n    def __init__(self, model_name='resnet50', pretrained=False):\n        super().__init__()\n        \n        self.num_class = 5\n        \n        \n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        \n        self.model.global_pool = nn.Sequential(AdaptiveConcatPool2d())\n        \n        self.model.fc = nn.Sequential(\n            nn.BatchNorm1d(4096),\n            nn.Dropout(.35),\n            nn.Linear(4096,512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(.35),\n            nn.Linear(512, self.num_class)\n            )\n        \n\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_train(model , loader , optim , loss_func):\n    \n    \n    trn_epoch_loss = 0\n    num_correct = 0\n    model.train()\n    for idx,(x,y) in enumerate(loader):\n        x_train , y_train = x.to(device) , y.to(device)\n        \n        optim.zero_grad()\n        z_train = model(x_train)    \n        loss = loss_func(z_train,y_train)\n        \n        loss.backward()\n        optim.step()\n            \n        pred_train = torch.argmax( z_train,1 )\n        num_correct += (pred_train == y_train).sum().item()        \n        trn_epoch_loss +=  loss.item()\n        \n    return trn_epoch_loss/len(loader),  num_correct/len(loader.dataset)\n\n\ndef run_valid( loader, model ):    \n    \n    model.eval()\n    pred_val = 0\n    num_correct = 0\n   # accuracy = 0         \n    lst_pred_val = list()\n    with torch.no_grad():       \n        for val_index,(x,y) in enumerate(loader):\n            \n            x_valid = x.to(device)\n            y_valid = y.to(device)\n            z_valid = model(x_valid)\n            \n            pred_val = torch.argmax( z_valid,1 )\n            #_, top_class = z_valid.topk(1, dim=1)\n            #equals = top_class == y_valid.view(*top_class.shape)\n            #accuracy += torch.mean(equals.type(torch.FloatTensor))\n            num_correct += (pred_val == y_valid).sum().item()  \n            \n            \n            lst_pred_val.append(pred_val.detach().cpu().numpy())\n        \n        predictions = np.concatenate(lst_pred_val)\n    \n    return predictions , num_correct/len(loader.dataset)#,accuracy/len(loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module): \n    def __init__(self, classes, smoothing=0.0, dim=-1): \n        super(LabelSmoothingLoss, self).__init__() \n        self.confidence = 1.0 - smoothing \n        self.smoothing = smoothing \n        self.cls = classes \n        self.dim = dim \n    def forward(self, pred, target): \n        pred = pred.log_softmax(dim=self.dim) \n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred) \n            true_dist.fill_(self.smoothing / (self.cls - 1)) \n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LOGGER.info(f'  ****Train Augmentaion**** \\n\\n {get_train_transform()} \\n\\n')\n\noof_labels = np.zeros((len(df_train)))\nfor fold_num in range(cfg.split):\n    \n    LOGGER.info(f\"==================== fold: {fold_num+1} training ====================\")\n    model_path = f\"F_264_resnet50_multihead{fold_num+1}.pth\"\n    \n    trn_idx = df_train[df_train['stf_Kfold'] != fold_num].index\n    val_idx = df_train[df_train['stf_Kfold'] == fold_num].index\n    \n    df_trn = df_train.loc[trn_idx,['image_id','label']].reset_index(drop=True)\n    df_val = df_train.loc[val_idx,['image_id','label']].reset_index(drop=True)\n    \n    ## train dataset\n\n    trainset = Train_dataset(df_trn ,transform = get_train_transform() )\n    train_loader = torch.utils.data.DataLoader( trainset ,shuffle=True , batch_size = cfg.batch_size )\n\n    ## valid dataset\n\n    validset = Train_dataset(df_val ,transform = get_valid_transform() )\n    valid_loader = torch.utils.data.DataLoader( validset ,shuffle=False , batch_size = cfg.batch_size )\n    \n    model = cnn_resnet50(cfg.model_name, pretrained=True).to(device)\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print('Number of params used',pytorch_total_params)\n    ## Defining optimizer and loss function\n\n    optimizer = Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, amsgrad=False)\n    #criterion = nn.CrossEntropyLoss()\n    criterion = LabelSmoothingLoss(classes=cfg.target_size, smoothing=cfg.smoothing).to(device)\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n    #scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='max', patience = 1 ,verbose=True, factor=0.2)\n\n    best_acc = 0\n    counter = 0\n    \n    for epoch in range(cfg.num_epochs):\n        \n        start_time = time.time()\n        \n        trn_loss,trn_acc    =  run_train(model , train_loader , optimizer , criterion)\n        predictions,val_acc = run_valid( valid_loader, model )\n        \n        train_time = str(datetime.timedelta(seconds=time.time() - start_time))[:7]           \n            \n        #scheduler.step(val_acc) --- Use for ReduceLROnPlateau \n        scheduler.step()\n        LOGGER.info(f'Epoch {epoch+1} | Train Loss: {trn_loss:.4f} | Train acc: {trn_acc:.4f} | Val Acc: {val_acc:.4f} | best_acc: {best_acc:.4f} | time: {train_time}')\n        if val_acc > best_acc:\n            print('saving best validation acc')\n            best_acc = val_acc\n            oof_labels[val_idx] = predictions.reshape(-1,)\n            torch.save(copy.deepcopy(model.state_dict()), model_path)\n                \n        else:\n            print('patience starts .........')\n            counter+=1\n            if (counter > cfg.patience):\n                LOGGER.info(f'Early stopping. Best correct accuracy: {best_acc:.4f}')\n                break;\n                \nprint('Training Completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = accuracy_score(df_train['label'].values, oof_labels)\nLOGGER.info(f'\\noof score : {oof:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dataframe\ndf_oof = pd.DataFrame({'original' : df_train['label'].values,\n              'pred' : oof_labels,})\ndf_oof.to_csv('oof_F_384_Resnet50_multihead.csv',index= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}